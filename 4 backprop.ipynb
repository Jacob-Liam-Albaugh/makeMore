{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Check if GPU is available\n",
    "output_device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n",
      "15\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
     ]
    }
   ],
   "source": [
    "names = open('names.txt','r').read().splitlines()\n",
    "print(len(names))\n",
    "print(max([len(name) for name in names]))\n",
    "print(names[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(names))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataset\n",
    "block_size = 5 # context length: # of characters taken for prediction\n",
    "\n",
    "def build_dataset(names):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for name in names:\n",
    "        context = [0] * block_size\n",
    "        for ch in name + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(names)\n",
    "n1 = int(0.8 * len(names))\n",
    "n2 = int(0.9 * len(names))\n",
    "\n",
    "Xtr, Ytr = build_dataset(names[:n1]) # training set 80%\n",
    "Xdev, Ydev = build_dataset(names[n1:n2]) # dev set 10%\n",
    "Xte, Yte = build_dataset(names[n2:]) # test set 10%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5417\n"
     ]
    }
   ],
   "source": [
    "n_emb = 10 # the dimensionality of the character embedding vectors\n",
    "n_hid = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_emb),         generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_emb * block_size, n_hid), generator=g) * (5/3)/((n_emb * block_size)**0.5)\n",
    "b1 = torch.randn(n_hid,                     generator=g) * 0.1 # not needed bc of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hid, vocab_size),         generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                  generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hid))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hid))*0.1\n",
    "\n",
    "parameters = [C, W1, W2, b1, b2, bngain, bnbias] # b1 not needed bc of BN\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minibatch contruction\n",
    "batch_size = n = 32\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.6037, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass, \"chunkated into smaller steps that are possible to backward one at a time\n",
    "\n",
    "emb = C[Xb] # embedding the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # b1 not needed bc of BN\n",
    "\n",
    "# BatchNorm layer (see pg.3 ALGO 1 in https://arxiv.org/pdf/1502.03167.pdf)\n",
    "bnmeani = (1/n) * hprebn.sum(0, keepdim=True) # mini-batch mean\n",
    "\n",
    "bndiff = hprebn - bnmeani # difference from mean\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1) * (bndiff2).sum(0, keepdim=True) #mini-batch variance (bessel's correction dvided by n-1 not n)\n",
    "\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv # normalize\n",
    "\n",
    "hpreact = bngain * bnraw + bnbias # scale and shift\n",
    "\n",
    "# nonlinearity\n",
    "h = torch.tanh(hpreact)\n",
    "\n",
    "# linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for stability of softmax & gradient\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdim=True)\n",
    "counts_sum_inv = counts_sum**-1 \n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# torch backward pass\n",
    "for p in parameters:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backprop through the whole thing manually, \n",
    "# backpropagating through exactly all of the variables \n",
    "# as they are defined in the forward pass above, one by one\n",
    "\n",
    "# softmax back prop\n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "dprobs = (1/probs) * dlogprobs\n",
    "\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts_sum = -counts_sum**-2 * dcounts_sum_inv\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum \n",
    "\n",
    "dnorm_logits = counts * dcounts # counts = norm_logits.exp()\n",
    "dlogits = dnorm_logits.clone()\n",
    "dlogit_maxes = -dnorm_logits.sum(1, keepdim=True)\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "\n",
    "# lienar back prop\n",
    "dh = dlogits @ W2.t()\n",
    "dW2 = h.t() @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "\n",
    "# nonlinearity back prop\n",
    "dhpreact = (1 - h**2) * dh # h = tanh(hpreact)\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "\n",
    "# BatchNorm back prop\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "dbnvar = (-0.5*(bnvar + 1e-5)**(-1.5)) * dbnvar_inv\n",
    "dbndiff2 = 1.0/(n-1) * torch.ones_like(bndiff2) * dbnvar # torch broadcasting taking care of the replication across dimentions\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbndiff += 2 * bndiff * dbndiff2\n",
    "dbnmeani = -dbndiff.sum(0, keepdim=True)\n",
    "dhprebn = dbndiff.clone()\n",
    "dhprebn += 1/n * torch.ones_like(hprebn) * dbnmeani\n",
    "\n",
    "# Linear back prop\n",
    "dembcat = dhprebn @ W1.t()\n",
    "dW1 = embcat.t() @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "demb = dembcat.view(emb.shape)\n",
    "\n",
    "# Embedding back prop\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += demb[k,j]\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6037213802337646 diff: 2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 7.683411240577698e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# softmax back prop\n",
    "\n",
    "dlogits = F.softmax(logits, 1) - F.one_hot(Yb, num_classes=logits.shape[1])\n",
    "dlogits /= n\n",
    "\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x156c1fc90>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dlogits.detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(7.1526e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# now:\n",
    "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16297\n",
      "      0/  20000: 3.4821\n",
      "  10000/  20000: 2.0529\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 20000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "\n",
    "  # kick off optimization\n",
    "  for i in range(max_steps):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    # Linear layer\n",
    "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "    # BatchNorm layer\n",
    "    # -------------------------------------------------------------\n",
    "    bnmean = hprebn.mean(0, keepdim=True)\n",
    "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    # -------------------------------------------------------------\n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "      p.grad = None\n",
    "    #loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "    # manual backprop! #swole_doge_meme\n",
    "    # -----------------\n",
    "    # my code\n",
    "    # -+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
    "    # softmax back prop\n",
    "    dlogits = F.softmax(logits, 1) - F.one_hot(Yb, num_classes=logits.shape[1])\n",
    "    dlogits /= n\n",
    "\n",
    "    # lienar back prop\n",
    "    dh = dlogits @ W2.t()\n",
    "    dW2 = h.t() @ dlogits\n",
    "    db2 = dlogits.sum(0)\n",
    "\n",
    "    # nonlinearity back prop\n",
    "    dhpreact = (1 - h**2) * dh # h = tanh(hpreact)\n",
    "    # batchnorm back prop\n",
    "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "\n",
    "    # Linear back prop\n",
    "    dembcat = dhprebn @ W1.t()\n",
    "    dW1 = embcat.t() @ dhprebn\n",
    "    db1 = dhprebn.sum(0)\n",
    "\n",
    "    # Embedding back prop\n",
    "    demb = dembcat.view(emb.shape)\n",
    "    dC = torch.zeros_like(C)\n",
    "    for k in range(Xb.shape[0]):\n",
    "        for j in range(Xb.shape[1]):\n",
    "            ix = Xb[k,j]\n",
    "            dC[ix] += demb[k,j]\n",
    "    # -+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "    # -----------------\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "    for p, grad in zip(parameters, grads):\n",
    "      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "    #if i >= 10000: # TODO: delete early breaking when you're ready to train the full net\n",
    "    #  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "all(): argument 'input' (position 1) must be Tensor, not bool",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# useful for checking your gradients\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p,g \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(parameters, grads):\n\u001b[0;32m----> 3\u001b[0m  \u001b[43mcmp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m, in \u001b[0;36mcmp\u001b[0;34m(s, dt, t)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcmp\u001b[39m(s, dt, t):\n\u001b[0;32m----> 3\u001b[0m   ex \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      4\u001b[0m   app \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mallclose(dt, t\u001b[38;5;241m.\u001b[39mgrad)\n\u001b[1;32m      5\u001b[0m   maxdiff \u001b[38;5;241m=\u001b[39m (dt \u001b[38;5;241m-\u001b[39m t\u001b[38;5;241m.\u001b[39mgrad)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mTypeError\u001b[0m: all(): argument 'input' (position 1) must be Tensor, not bool"
     ]
    }
   ],
   "source": [
    "# useful for checking your gradients\n",
    "# for p,g in zip(parameters, grads):\n",
    "#  cmp(str(tuple(p.shape)), g, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[Xtr]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.15762996673584\n",
      "val 2.1818134784698486\n"
     ]
    }
   ],
   "source": [
    "# evaluate train and val loss\n",
    "\n",
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 + b1\n",
    "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I achieved:\n",
    "# train 2.0718822479248047\n",
    "# val 2.1162495613098145"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carlah\n",
      "quilla\n",
      "khirmis\n",
      "jehty\n",
      "salana\n",
      "corron\n",
      "nalee\n",
      "arian\n",
      "quigne\n",
      "marahceriiv\n",
      "kaleig\n",
      "dham\n",
      "joce\n",
      "quintis\n",
      "linah\n",
      "jadiquwatero\n",
      "diarisi\n",
      "jaceelins\n",
      "bred\n",
      "eliia\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)      \n",
    "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "      hpreact = embcat @ W1 + b1\n",
    "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "      logits = h @ W2 + b2 # (N, vocab_size)\n",
    "      # sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      if ix == 0:\n",
    "        break\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
